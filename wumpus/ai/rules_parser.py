from enum import Enum
import re
from dataclasses import dataclass
from typing import List, Union

# This piece of code is AI-generated by DeepSeek V3
# It mostly involve compilers design principles
# Which parse plain text into a structured format
# It's a helper to easier write rules in plain text then convert automatically
# Instead of hardcoding rules in the codebase 

# Prompt: Implement a parser, that parse Predicate, and/or, =>,... (first order logics) from plain text into python class. You can use | for or => for predicates and & for and and so on

@dataclass
class Predicate:
    name: str
    args: List[str]
    
    def __repr__(self):
        return f"{self.name}({','.join(self.args)})"

@dataclass
class Not:
    expr: 'LogicExpr'
    
    def __repr__(self):
        return f"!{self.expr}"

@dataclass
class And:
    left: 'LogicExpr'
    right: 'LogicExpr'
    
    def __repr__(self):
        return f"({self.left} & {self.right})"

@dataclass
class Or:
    left: 'LogicExpr'
    right: 'LogicExpr'
    
    def __repr__(self):
        return f"({self.left} | {self.right})"

@dataclass
class Implies:
    left: 'LogicExpr'
    right: 'LogicExpr'
    
    def __repr__(self):
        return f"({self.left} => {self.right})"

LogicExpr = Union[Predicate, Not, And, Or, Implies]

class LogicParser:
    def __init__(self):
        self.token_patterns = [
            (r'=>', 'IMPLIES'),
            (r'&', 'AND'),
            (r'\|', 'OR'),
            (r'!', 'NOT'),
            (r'\(', 'LPAREN'),
            (r'\)', 'RPAREN'),
            (r'\,', 'COMMA'),
            # (r'[a-zA-Z][a-zA-Z0-9_]*', 'IDENTIFIER'),
            # (r'[0-9]+', 'NUMBER'),
            #(r'([a-zA-Z][a-zA-Z0-9_]*|[0-9]+)', 'ARG'),  # Changed to ARG
            (r'([a-zA-Z][a-zA-Z0-9_]*|[0-9]+|\+|\-|\*|\/)', 'ARG'),
            (r'\s+', 'WHITESPACE')
        ]
        self.token_re = re.compile('|'.join(f'(?P<{name}>{pattern})' for pattern, name in self.token_patterns))
    
    def tokenize(self, text):
        tokens = []
        for match in self.token_re.finditer(text):
            token_type = match.lastgroup
            if token_type == 'WHITESPACE':
                continue
            token_value = match.group()
            tokens.append((token_type, token_value))
        return tokens
    
    def parse(self, text) -> LogicExpr:
        self.tokens = self.tokenize(text)
        # print(f"Tokens: {self.tokens}")
        self.current_token = 0
        return self.parse_expression()
    
    def parse_expression(self)-> LogicExpr:
        return self.parse_implies()
    
    def parse_implies(self):
        left = self.parse_or()
        
        while self.peek() == 'IMPLIES':
            self.consume('IMPLIES')
            right = self.parse_or()
            left = Implies(left, right)
        
        return left
    
    def parse_or(self):
        left = self.parse_and()
        
        while self.peek() == 'OR':
            self.consume('OR')
            right = self.parse_and()
            left = Or(left, right)
        
        return left
    
    def parse_and(self):
        left = self.parse_not()
        
        while self.peek() == 'AND':
            self.consume('AND')
            right = self.parse_not()
            left = And(left, right)
        
        return left
    
    def parse_not(self):
        if self.peek() == 'NOT':
            self.consume('NOT')
            expr = self.parse_atom()
            return Not(expr)
        return self.parse_atom()

    def parse_atom(self):
        if self.peek() == 'LPAREN':
            self.consume('LPAREN')
            expr = self.parse_expression()
            self.consume('RPAREN')
            return expr

    
        # Handle predicates
        ident = self.consume('ARG')
        if self.peek() == 'LPAREN':
            self.consume('LPAREN')
            args = []
            current_arg = []
            
            # Collect all tokens until RPAREN or COMMA
            while True:
                if self.peek() in ['RPAREN', 'COMMA']:
                    # Add completed argument
                    if current_arg:
                        args.append(''.join(current_arg))
                        current_arg = []
                    if self.peek() == 'RPAREN':
                        break
                    self.consume('COMMA')
                    continue
                
                # Collect all ARG tokens for current argument
                token_type, token_value = self.consume('ARG')
                current_arg.append(token_value)
            
            self.consume('RPAREN')
            return Predicate(ident[1], args)
        
        return Predicate(ident[1], [])  # Propositional case

    
    def peek(self):
        if self.current_token < len(self.tokens):
            return self.tokens[self.current_token][0]
        return None
    
    def consume(self, expected_type):
        if self.current_token >= len(self.tokens):
            raise ValueError(f"Expected {expected_type} but got EOF")
        
        token_type, token_value = self.tokens[self.current_token]
        if token_type != expected_type:
            raise ValueError(f"Expected {expected_type} but got {token_type}")
        
        self.current_token += 1
        return (token_type, token_value)

# Example usage
if __name__ == "__main__":
    parser = LogicParser()

    expr1 = parser.parse("Pit(x,y) => Breeze(x,y)")
    expr2 = parser.parse("Breeze(1,2) & Stench(1,2)")
    expr3 = parser.parse("Gold(3,3) | (Pit(2,2) & Wumpus(1,3))")
    expr4 = parser.parse("!Pit(1,1)")
    expr5 = parser.parse("Breeze(1,1)")
    expr6 = parser.parse("Breeze(x,y+1)")

    # getting components of the expressions
    print(expr1.left) # Pit(x,y)
    print(expr5.args)  # ['1', '1']
    print(expr6.args)  # ['x', 'y+1']

    # check type of expression
    print(isinstance(expr4, Not))  # True
    print(expr3.__class__.__name__)  # Or


    expr7 = parser.parse("Breeze(x,y) => Pit(x+1,y) | Pit(x-1,y) | Pit(x,y+1) | Pit(x,y-1)")
    print(expr7)  # Breeze(x,y) => (Pit(x+1,y) | Pit(x-1,y) | Pit(x,y+1) | Pit(x,y-1))